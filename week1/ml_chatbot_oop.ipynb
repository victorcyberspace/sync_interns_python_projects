class CustomCallback(tf.keras.callbacks.Callback):

  # initialize the custom callback with the directory path
  def __init__(self, directory_path):

    # call the parent class constructor
    super().__init__()  

    # store the directory path
    self.directory_path = directory_path  

  # define a callback function to be called at the end of each epoch
  def on_epoch_end(self, epoch, logs=None):
    
    # start time measurement 
    start_time = time.time()

    # if the current epoch is divisible by 50:
    if epoch % 50 == 0:

      # save the model to a file with epoch number in the name
      self.model.save(os.path.join(self.directory_path, f'chatbot_model_epoch_{epoch}.model'))

      # end time measurement
      end_time = time.time()

      # find the time taken 
      execution_time = end_time - start_time

      # print training progress information
      print(f"time: {execution_time:.2f}s  epoch: {epoch}/{self.params['epochs']}  loss: {logs['loss']:.2%}   accuracy: {logs['accuracy']:.2%}")

      # print a separator for visual clarity
      print('^' * 100)

      print(' ')  # print an empty line for spacing

    


# usage
intents_path = '/content/drive/MyDrive/ai_chatbot_files_OOP/intents.json'
directory_path = "/content/drive/MyDrive/ai_chatbot_files_OOP"

chatbot_model = ChatbotModel(intents_path, directory_path)
intents = chatbot_model.load_intents()
chatbot_model.preprocess_data(intents)

# specify the directory path
os.makedirs(directory_path, exist_ok=True)

# save words.pkl
chatbot_model.save_pickle(chatbot_model.words, os.path.join(directory_path, 'words.pkl'))

# save classes.pkl
chatbot_model.save_pickle(chatbot_model.classes, os.path.join(directory_path, 'classes.pkl'))

chatbot_model.words = [chatbot_model.lemmatizer.lemmatize(word) for word in chatbot_model.words if word not in chatbot_model.ignore_letters]
chatbot_model.words = sorted(set(chatbot_model.words))
chatbot_model.classes = sorted(set(chatbot_model.classes))

# process training data
for document in chatbot_model.documents:

  # create an empty bag-of-words representation
  bag = []

  # get word patterns from the document
  word_patterns = document[0]

  # lemmatize and lowercase words in the patterns
  word_patterns = [chatbot_model.lemmatizer.lemmatize(word.lower()) for word in word_patterns]

  # create bag-of-words representation for the document
  for word in chatbot_model.words:

    # add 1 if word is present, 0 otherwise
    bag.append(1) if word in word_patterns else bag.append(0)  

    # create output row with one-hot encoding for the class
    # start with empty output row
    output_row = list(chatbot_model.output_empty)  

    # initialize with zeros
    output_row = [0] * len(chatbot_model.classes) 

    # set the index corresponding to the class to 1
    output_row[chatbot_model.classes.index(document[1])] = 1  

    # append bag-of-words and output row to training data
    chatbot_model.training.append([bag, output_row])

# shuffle training data randomly
random.shuffle(chatbot_model.training)

# convert training data to a NumPy array
chatbot_model.training = np.array(chatbot_model.training, dtype=object)

# extract training features (X) and labels (y)
chatbot_model.train_x = list(chatbot_model.training[:, 0])  # get all rows, first column (bag-of-words)
chatbot_model.train_y = list(chatbot_model.training[:, 1])  # get all rows, second column (output labels)
